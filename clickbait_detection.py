# -*- coding: utf-8 -*-
"""clickbait_detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16yBwlF12eiHm9tguaDwnGJF52hdQxlys

#Clickbait Detection
Testing the best classifier for clickbait detection

##Import the libraries
"""

!pip install simpletransformers

import numpy as np
import pandas as pd
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import CountVectorizer
from nltk.probability import FreqDist
import string as s
import re
from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_curve, roc_auc_score
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.naive_bayes import MultinomialNB
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split, GridSearchCV, KFold
import matplotlib.pyplot as plt
import seaborn as sns
from simpletransformers.classification import ClassificationModel
import logging
from google.colab import drive
drive.mount('/content/gdrive')

import nltk
nltk.download('stopwords')
nltk.download('punkt')
nltk.download('wordnet')

"""##Load the dataset"""

click=pd.read_csv('/content/gdrive/MyDrive/TextMiningDatasets/clickbait.csv')
click

"""##Drop index column"""

click= click.drop(click.columns[0], axis=1)
click

"""##Analyze the dataset"""

click.dtypes

click.isna().sum()

click['label'].value_counts()

sns.countplot(x ='label', data = click)
plt.show()

"""##Split into training and testing sets"""

X=click.text
X

Y=click.label
Y

"""##Tokenize"""

def tokenization(text):
    lst=text.split()
    return lst
X=X.apply(tokenization)
print(X)

"""##Remove stopwords"""

def remove_stopwords(lst):
    stop=stopwords.words('english')
    new_lst=[]
    for i in lst:
        if i.lower() not in stop:
            new_lst.append(i)
    return new_lst
X=X.apply(remove_stopwords)
print(X)

"""##Convert to lowercase"""

def lowercase_conversion(lst):
    new_lst=[]
    for i in lst:
        i=i.lower()
        new_lst.append(i)
    return new_lst
X=X.apply(lowercase_conversion)
print(X)

"""##Remove numbers"""

def number_removal(lst):
    nonum_lst=[]
    new_lst=[]
    for i in lst:
        for j in s.digits:
            i=i.replace(j,'')
        nonum_lst.append(i)
    for i in nonum_lst:
        if i!='':
            new_lst.append(i)
    return new_lst
X=X.apply(number_removal)
print(X)

"""##Remove punctuation"""

def remove_punctuation(lst):
    new_lst=[]
    for i in lst:
        for j in s.punctuation:
            i=i.replace(j,'')
        new_lst.append(i)
    return new_lst
X=X.apply(remove_punctuation)
print(X)

"""##Remove additional space"""

def remove_space(lst):
    new_lst=[]
    for i in lst:
        i=i.strip()
        new_lst.append(i)
    return new_lst
X=X.apply(remove_space)
print(X)

"""##Lemmatize"""

lemmatizer = WordNetLemmatizer()

def lemmatize(lst):
    new_lst=[]
    for i in lst:
        i=lemmatizer.lemmatize(i)
        new_lst.append(i)
    return new_lst
X=X.apply(lemmatize)
print(X)

X= X.apply(lemmatize).apply(lambda x: ' '.join(x))
print(X)

"""##Apply Countvectorizer"""

cov=CountVectorizer(analyzer='word', ngram_range=(1,2),max_features=22500)
X=cov.fit_transform(X)

X=X.toarray()

feature_names = cov.get_feature_names_out()

pd.DataFrame(X, columns=feature_names)

"""##Multinomial Naive Bayes"""

repeats=30
accuracies=[]
fscore=[]

for i in range(0, repeats):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, stratify=click['label'])

  mnb=MultinomialNB()
  mnb.fit(X_train, y_train)

  y_pred=mnb.predict(X_test)
  accuracies.append(accuracy_score(y_test, y_pred))
  fscore.append(f1_score(y_test, y_pred, pos_label=1))

print(accuracies)
print(fscore)

print('Mean accuracy', np.mean(accuracies))
print("Standard deviation", np.std(accuracies))
print("F1-score", np.mean(fscore))
cm=confusion_matrix(y_test, y_pred)

cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

dy_pred_prob=mnb.predict_proba(X_test)[::, 1]
fpr, tpr, _=roc_curve(y_test, dy_pred_prob)
auc=roc_auc_score(y_test, dy_pred_prob)

plt.plot(fpr, tpr, label="AUC_MNB= "+ str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Postive Rate')
plt.legend(loc=4)
plt.show()

"""##Logistic Regression"""

repeats=30
accuracies=[]
fscore=[]

for i in range(0, repeats):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, stratify=click['label'])

  lr=LogisticRegression()
  lr.fit(X_train, y_train)

  y_pred=lr.predict(X_test)
  accuracies.append(accuracy_score(y_test, y_pred))
  fscore.append(f1_score(y_test, y_pred, pos_label=1))

print(accuracies)
print(fscore)

print('Mean accuracy', np.mean(accuracies))
print("Standard deviation", np.std(accuracies))
print("F1-score", np.mean(fscore))
cm=confusion_matrix(y_test, y_pred)

cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

dy_pred_prob=lr.predict_proba(X_test)[::, 1]
fpr, tpr, _=roc_curve(y_test, dy_pred_prob)
auc=roc_auc_score(y_test, dy_pred_prob)

plt.plot(fpr, tpr, label="AUC_LR= "+ str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Postive Rate')
plt.legend(loc=4)
plt.show()

"""##Support Vector Machines"""

repeats=30
accuracies=[]
fscore=[]

for i in range(0, repeats):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, stratify=click['label'])

  svm=SVC(probability=True)
  svm.fit(X_train, y_train)

  y_pred=svm.predict(X_test)
  accuracies.append(accuracy_score(y_test, y_pred))
  fscore.append(f1_score(y_test, y_pred, pos_label=1))

print(accuracies)
print(fscore)

print('Mean accuracy', np.mean(accuracies))
print("Standard deviation", np.std(accuracies))
print("F1-score", np.mean(fscore))
cm=confusion_matrix(y_test, y_pred)

cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

dy_pred_prob=svm.predict_proba(X_test)[::, 1]
fpr, tpr, _=roc_curve(y_test, dy_pred_prob)
auc=roc_auc_score(y_test, dy_pred_prob)

plt.plot(fpr, tpr, label="AUC_SVM= "+ str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Postive Rate')
plt.legend(loc=4)
plt.show()

"""##k-Nearest Neighbours"""

repeats=30
accuracies=[]
fscore=[]

for i in range(0, repeats):
  X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, stratify=click['label'])

  knn = KNeighborsClassifier(n_neighbors=5)
  knn.fit(X_train, y_train)

  y_pred=knn.predict(X_test)
  accuracies.append(accuracy_score(y_test, y_pred))
  fscore.append(f1_score(y_test, y_pred, pos_label=1))

print(accuracies)
print(fscore)

print('Mean accuracy', np.mean(accuracies))
print("Standard deviation", np.std(accuracies))
print("F1-score", np.mean(fscore))
cm=confusion_matrix(y_test, y_pred)

cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

dy_pred_prob=knn.predict_proba(X_test)[::, 1]
fpr, tpr, _=roc_curve(y_test, dy_pred_prob)
auc=roc_auc_score(y_test, dy_pred_prob)

plt.plot(fpr, tpr, label="AUC_knn= "+ str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Postive Rate')
plt.legend(loc=4)
plt.show()

"""##Random Forest"""

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42, stratify=click['label'])

param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'max_depth': [None, 10, 20],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': ['sqrt', 'log2']
}

rf_classifier = RandomForestClassifier(random_state=42)

grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, scoring='accuracy', cv=5)

grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_

best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Best Parameters:", best_params)
print("Test Accuracy:", accuracy)

repeats = 30
accuracies = []
fscores = []

for i in range(repeats):
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, stratify=click['label'])
    random_forest = RandomForestClassifier(n_estimators=150, max_depth=None, max_features='sqrt', min_samples_leaf=2, min_samples_split=10)
    random_forest.fit(X_train, y_train)
    y_pred = random_forest.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    fscore = f1_score(y_test, y_pred, pos_label=1)
    accuracies.append(accuracy)
    fscores.append(fscore)

print("Accuracies:", accuracies)
print("F-scores:", fscores)

print('Mean accuracy', np.mean(accuracies))
print("Standard deviation", np.std(accuracies))
print("F1-score", np.mean(fscore))
cm=confusion_matrix(y_test, y_pred)

cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

dy_pred_prob=random_forest.predict_proba(X_test)[::, 1]
fpr, tpr, _=roc_curve(y_test, dy_pred_prob)
auc=roc_auc_score(y_test, dy_pred_prob)

plt.plot(fpr, tpr, label="AUC_RF= "+ str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Postive Rate')
plt.legend(loc=4)
plt.show()

"""##Gradient Boosting"""

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=42, stratify=click['label'])

param_grid = {
    'n_estimators': [50, 100, 150, 200],
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 5, 7],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'subsample': [0.8, 1.0]
}

gb_classifier = GradientBoostingClassifier(random_state=42)
grid_search = GridSearchCV(estimator=gb_classifier, param_grid=param_grid, scoring='accuracy', cv=5)

grid_search.fit(X_train, y_train)
best_params = grid_search.best_params_
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)

print("Best Parameters:", best_params)
print("Test Accuracy:", accuracy)

repeats = 30
accuracies = []
fscores = []

for i in range(repeats):
    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, shuffle=True, stratify=click['label'])
    xgb = GradientBoostingClassifier(n_estimators=50, subsample=0.8, min_samples_split=2, min_samples_leaf=2, max_depth=7, learning_rate=0.2)
    xgb.fit(X_train, y_train)
    y_pred = xgb.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    fscore = f1_score(y_test, y_pred, pos_label=1)
    accuracies.append(accuracy)
    fscores.append(fscore)

print("Accuracies:", accuracies)
print("F-scores:", fscores)

print('Mean accuracy', np.mean(accuracies))
print("Standard deviation", np.std(accuracies))
print("F1-score", np.mean(fscore))
cm=confusion_matrix(y_test, y_pred)

cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [False, True])
cm_display.plot()
plt.show()

dy_pred_prob=xgb.predict_proba(X_test)[::, 1]
fpr, tpr, _=roc_curve(y_test, dy_pred_prob)
auc=roc_auc_score(y_test, dy_pred_prob)

plt.plot(fpr, tpr, label="AUC_XGB= "+ str(auc))
plt.ylabel('True Positive Rate')
plt.xlabel('False Postive Rate')
plt.legend(loc=4)
plt.show()

"""##Simple Transformers Classifier"""

click_train=click.iloc[:376, :]
click_train

click_train.columns = ["text", "labels"]
click_train

click_test=click.iloc[376:, :]
click_test

model = ClassificationModel('roberta', 'roberta-base', use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True})
model.train_model(click_train)
result, model_outputs, wrong_predictions = model.eval_model(click_test, acc=accuracy_score)

print(result)

n=5
kf = KFold(n_splits=n, shuffle=True)
results = []

for train_index, val_index in kf.split(click_train):
    click_train=click.iloc[:376, :]
    click_train.columns = ["text", "labels"]
    click_test=click.iloc[376:, :]

    model = ClassificationModel('bert', 'bert-base-uncased', use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True})
    model.train_model(click_train)
    result, model_outputs, wrong_predictions = model.eval_model(click_test, acc=accuracy_score)
    print(result['acc'])
    results.append(result['acc'])
print(results)

print("Mean accuracy:", np.mean(results))

"""##LSTM"""

from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, GlobalMaxPooling1D
from tensorflow.keras.models import Sequential
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping
from tensorflow.keras.preprocessing.sequence import pad_sequences

text = click['text'].values
labels = click['label'].values
X_train, X_test, y_train, y_test = train_test_split(text, labels, test_size=0.25, shuffle=True, stratify=click['label'])
print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)

vocab_size = 5000
maxlen = 300
embedding_size = 32

tokenizer = Tokenizer(num_words=vocab_size)
tokenizer.fit_on_texts(text)

X_train = tokenizer.texts_to_sequences(X_train)
x_test = tokenizer.texts_to_sequences(X_test)

X_train = pad_sequences(X_train, maxlen=maxlen)
x_test = pad_sequences(x_test, maxlen=maxlen)

tokenizer = Tokenizer()
tokenizer.fit_on_texts(X_train)

X_train_sequences = tokenizer.texts_to_sequences(X_train)
X_test_sequences = tokenizer.texts_to_sequences(X_test)

vocab_size = len(tokenizer.word_index) + 1
maxlen = 32

X_train_padded = pad_sequences(X_train_sequences, maxlen=maxlen, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=maxlen, padding='post')

embedding_size = 100

model = Sequential()
model.add(Embedding(vocab_size, embedding_size, input_length=maxlen))
model.add(LSTM(32, return_sequences=True))
model.add(GlobalMaxPooling1D())
model.add(Dropout(0.2))
model.add(Dense(1, activation='sigmoid'))

model.summary()

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
history = model.fit(X_train_padded, y_train, batch_size=32, validation_data=(X_test_padded, y_test), epochs=12)

plt.plot(history.history['accuracy'], label='accuracy')
plt.plot(history.history['val_accuracy'], label='test_accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.ylim([0, 1])
plt.legend(loc='lower right')

"""##Testing more simple transformers"""

import torch
cuda_available = torch.cuda.is_available()

model = ClassificationModel('electra', 'google/electra-base-discriminator', use_cuda=cuda_available, args={'reprocess_input_data': True, 'overwrite_output_dir': True})
model.train_model(click_train)
result, model_outputs, wrong_predictions = model.eval_model(click_test, acc=accuracy_score)

print(result)

n = 5
kf = KFold(n_splits=n, shuffle=True)
results = []

for train_index, val_index in kf.split(click):
    click_train = click.iloc[train_index, :]
    click_val = click.iloc[val_index, :]
    click_train.columns = ["text", "labels"]
    click_val.columns = ["text", "labels"]
    model = ClassificationModel('bert', 'bert-base-uncased', use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 3})
    model.train_model(click_train)
    result, _, _ = model.eval_model(click_val, acc=accuracy_score)
    print(result['acc'])
    results.append(result['acc'])

print(results)

print("Mean accuracy with 3 epochs:", np.mean(results))

n = 5
kf = KFold(n_splits=n, shuffle=True)
results = []

for train_index, val_index in kf.split(click):
    click_train = click.iloc[train_index, :]
    click_val = click.iloc[val_index, :]
    click_train.columns = ["text", "labels"]
    click_val.columns = ["text", "labels"]
    model = ClassificationModel('roberta', 'roberta-base', use_cuda=False, args={'reprocess_input_data': True, 'overwrite_output_dir': True, 'num_train_epochs': 3})
    model.train_model(click_train)
    result, _, _ = model.eval_model(click_val, acc=accuracy_score)
    print(result['acc'])
    results.append(result['acc'])

print(results)

print(result)